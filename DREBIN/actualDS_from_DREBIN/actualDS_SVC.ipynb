{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b59630",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc1789ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def load_features(fname, shas=False):\n",
    "    \"\"\"Load feature set. \n",
    "\n",
    "    Args:\n",
    "        feature_set (str): The common prefix for the dataset. \n",
    "            (e.g., 'data/features/drebin' -> 'data/features/drebin-[X|Y|meta].json')\n",
    "\n",
    "        shas (bool): Whether to include shas. In some versions of the dataset, \n",
    "            shas were included to double-check alignment - these are _not_ features \n",
    "            and _must_ be removed before training. \n",
    "    \n",
    "    Returns:\n",
    "        Tuple[List[Dict], List, List]: The features, labels, and timestamps \n",
    "            for the dataset. \n",
    "\n",
    "    \"\"\"\n",
    "    logging.info('Loading features...')\n",
    "    with open('{}-X.json'.format(fname), 'r') as f:\n",
    "        X = json.load(f)\n",
    "    # if not shas:\n",
    "    #     [o.pop('sha256') for o in X]\n",
    "\n",
    "    logging.info('Loading labels...')\n",
    "    with open('{}-Y.json'.format(fname), 'rt') as f:\n",
    "        y = json.load(f)\n",
    "#     if 'apg' not in fname:\n",
    "#         y = [o[0] for o in y]\n",
    "\n",
    "    logging.info('Loading timestamps...')\n",
    "    with open('{}-meta.json'.format(fname), 'rt') as f:\n",
    "        t = json.load(f)\n",
    "    t = [o['dex_date'] for o in t]\n",
    "    if 'apg' not in fname:\n",
    "        t = [datetime.strptime(o if isinstance(o, str) else time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(o)),\n",
    "                               '%Y-%m-%dT%H:%M:%S') for o in t]\n",
    "    else:\n",
    "        t = [datetime.strptime(o if isinstance(o, str) else time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(o)),\n",
    "                               '%Y-%m-%d %H:%M:%S') for o in t]\n",
    "    return X, y, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "069dc055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "#from load_features import *\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "X, y, t = load_features(os.path.join(os.getcwd(), 'actualDS'))\n",
    "\n",
    "\n",
    "vec = DictVectorizer()\n",
    "Xv = vec.fit_transform(X)\n",
    "yv = np.asarray(y)\n",
    "tv = np.asarray(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f33b0d6",
   "metadata": {},
   "source": [
    "# Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b82a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "590ea56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [38:53<00:00, 233.32s/it]\n"
     ]
    }
   ],
   "source": [
    "num_test = 10\n",
    "report_history = []\n",
    "for i in tqdm(range(num_test)):\n",
    "    Xv_tr, Xv_test, yv_tr, yv_test, tv_tr, tv_test = \\\n",
    "        train_test_split(Xv, yv, tv, test_size=0.33)\n",
    "\n",
    "    drebin_lsvc = SVC(kernel='linear', C=1)\n",
    "    drebin_lsvc.fit(Xv_tr, yv_tr)\n",
    "    yv_tr_pred = drebin_lsvc.predict(Xv_tr)\n",
    "    yv_test_pred = drebin_lsvc.predict(Xv_test)\n",
    "    report_history.append({\"train\":classification_report(yv_tr, yv_tr_pred, digits=6,\n",
    "                                                         output_dict=True),\n",
    "                           \"test\":classification_report(yv_test, yv_test_pred,digits=6,\n",
    "                                                         output_dict=True)})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017aefb5",
   "metadata": {},
   "source": [
    "## Average result on 10 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "877f5624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def get_avg_dict(report_history):\n",
    "    nh = len(report_history)\n",
    "    # compute avg_report\n",
    "    avg_report = deepcopy(report_history[0])\n",
    "    for k in avg_report['train'].keys():\n",
    "        if k == 'accuracy':\n",
    "            avg_report['train'][k] = np.mean([report_history[i]['train'][k] for i in range(nh)])\n",
    "            avg_report['test'][k] = np.mean([report_history[i]['test'][k] for i in range(nh)])\n",
    "        else:\n",
    "            for score in avg_report['train'][k].keys():\n",
    "                avg_report['train'][k][score] = \\\n",
    "                    np.mean([report_history[i]['train'][k][score] for i in range(nh)])\n",
    "                avg_report['test'][k][score] = \\\n",
    "                    np.mean([report_history[i]['test'][k][score] for i in range(nh)])\n",
    "    return avg_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cb95f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': {'0': {'precision': 0.9995788418931333, 'recall': 0.9997946008583162, 'f1-score': 0.9996867040520272, 'support': 28722.5}, '1': {'precision': 0.9979454372635559, 'recall': 0.9957970690804177, 'f1-score': 0.9968695304147586, 'support': 2879.5}, 'accuracy': 0.9994304157964686, 'macro avg': {'precision': 0.9987621395783446, 'recall': 0.9977958349693669, 'f1-score': 0.998278117233393, 'support': 31602.0}, 'weighted avg': {'precision': 0.9994301504766341, 'recall': 0.9994304157964686, 'f1-score': 0.9994301103309506, 'support': 31602.0}}, 'test': {'0': {'precision': 0.9889834917924756, 'recall': 0.9915260627834759, 'f1-score': 0.9902528382037387, 'support': 14160.5}, '1': {'precision': 0.9123453920282758, 'recall': 0.888775267049312, 'f1-score': 0.9003778524781682, 'support': 1405.5}, 'accuracy': 0.982243350892972, 'macro avg': {'precision': 0.9506644419103758, 'recall': 0.940150664916394, 'f1-score': 0.9453153453409534, 'support': 15566.0}, 'weighted avg': {'precision': 0.9820689076372761, 'recall': 0.982243350892972, 'f1-score': 0.9821377542024392, 'support': 15566.0}}}\n"
     ]
    }
   ],
   "source": [
    "avg_report = get_avg_dict(report_history) \n",
    "print(avg_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fe71ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open(\"actualDS_classification_report.json\", \"w\") as f:\n",
    "    avg_report = get_avg_dict(report_history)\n",
    "    json.dump(avg_report,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
