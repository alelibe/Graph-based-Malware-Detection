import torch
from Utils.vocabulary import Vocabulary
from Utils.config import external_fun_vocabulary,preprocess_path
from model.EmbedderGraphModel import EmbedderGraphModel
from Utils.preprocessedDataset import PreprocessedDataset,create_batch_data
from torch_geometric.loader import DataLoader
from torcheval.metrics.functional import binary_f1_score
from torch import nn
from tqdm import tqdm

import logging
logging.getLogger().setLevel(logging.ERROR)


def train_one_epoch(model,optimizer: torch.optim.Adam,loss: torch.nn.modules.loss,training_loader,epoch:int):
    # this function trains just one epoch at time
    local_device = torch.device("cpu")
    loss_history = []
    f1_history = []
    # iterate over all the batches in the data loader
    pbar = tqdm(training_loader, position=0, leave=True)
    pbar.set_postfix({'training__loss': 0.0,
                      'training__f1_score': 0.0})
    for _idx_bt, _batch in enumerate(pbar):
    # for _batch in tqdm(training_loader):
        model.train()
        _real_batch, _position, _hash, _external_list, _function_edges, _classes = create_batch_data(one_batch=_batch)
        
        _real_batch = _real_batch.to(local_device)
        _position = torch.tensor(_position, dtype=torch.long).cpu()
        _classes = _classes.float().cpu()

        # forward pass
        train_batch_pred = model(local_method_batch=_real_batch, local_bt_positions=_position, 
                                 bt_external_names=_external_list, bt_all_method_edges=_function_edges, 
                                 local_device=local_device)
        # remove dimension 1 if needed 
        if len(train_batch_pred) > 1:
            train_batch_pred = train_batch_pred.squeeze()
        else:
            train_batch_pred = train_batch_pred[0]
        # print(train_batch_pred, "\n",_classes)
        # compute loss function for the current batch
        loss_batch = loss(train_batch_pred, _classes)
        loss_mean = sum(loss_history)/(_idx_bt+1)
        # compute f1 score for the current batch
        f1_batch = binary_f1_score(train_batch_pred, _classes)
        f1_mean = sum(f1_history)/(_idx_bt+1)
        pbar.set_postfix({'training__loss': float(loss_mean),
                          'training__f1_score': float(f1_mean)})
        pbar.update()

        # store their histories
        loss_history.append(loss_batch)
        f1_history.append(f1_batch)
        
        optimizer.zero_grad()
        # backward pass
        loss_batch.backward()
        # update the optimizer
        optimizer.step()

    print(f"epoch {epoch}: loss {loss_mean} | f1_score {f1_mean}")
    return loss_mean,f1_mean

def train_model():
    max_vocab_size = 10000
    vocab = Vocabulary(freq_file=external_fun_vocabulary, max_vocab_size=max_vocab_size)

    model = EmbedderGraphModel(external_vocab=vocab)

    # loss function
    binary_cross_entropy_loss = nn.BCELoss()
    
    lr = 1e-3
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    max_epochs = 3
    
    dataset_root_path = preprocess_path
    train_batch_size = 256
    test_batch_size = 32


    train_dataset = PreprocessedDataset(root_dataset=dataset_root_path, flag='training')
    training_loader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=False)

    # valid_dataset = PreprocessedDataset(root=dataset_root_path, flag="valid")
    # valid_loader = DataLoader(dataset=valid_dataset, batch_size=test_batch_size)

    for epoch in range(max_epochs):
        # train for one epoch

        train_one_epoch(model,optimizer,binary_cross_entropy_loss,training_loader,epoch)
        



if __name__ == '__main__': 
    train_model()