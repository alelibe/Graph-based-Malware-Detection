import torch
from Utils.vocabulary import Vocabulary
from Utils.config import external_fun_vocabulary,preprocess_path
from model.EmbedderGraphModel import EmbedderGraphModel
from Utils.preprocessedDataset import PreprocessedDataset,create_batch_data
from torch_geometric.loader import DataLoader
from torcheval.metrics.functional import binary_f1_score,auc
from torch import nn
from tqdm import tqdm

import logging
logging.getLogger().setLevel(logging.ERROR)


def validate(model, loss: torch.nn.modules.loss, valid_loader, epoch:int):
    """
    """
    local_device = torch.device("cpu")
    loss_history     = []
    f1_history       = []
    auc_history = []

    # iterate over all the batches in the data loader
    pbar = tqdm(valid_loader, position=0, leave=True)
    pbar.set_postfix({'validation__loss': 0.0,
                      'validation__f1_score': 0.0,
                      'validation__auc': 0.0})
    
    for _idx_bt, _batch in enumerate(pbar):
    # for _batch in tqdm(training_loader):
        model.eval()
        _real_batch, _position, _hash, _external_list, _function_edges, _classes = create_batch_data(one_batch=_batch)
        
        _real_batch = _real_batch.to(local_device)
        _position = torch.tensor(_position, dtype=torch.long).cpu()
        _classes = _classes.float().cpu()

        # forward pass
        train_batch_pred = model(local_method_batch=_real_batch, local_bt_positions=_position, 
                                 bt_external_names=_external_list, bt_all_method_edges=_function_edges, 
                                 local_device=local_device)
        # remove dimension 1 if needed 
        if len(train_batch_pred) > 1:
            train_batch_pred = train_batch_pred.squeeze()
        else:
            train_batch_pred = train_batch_pred[0]

        # compute loss function for the current batch
        loss_batch = loss(train_batch_pred, _classes)
        loss_history.append(loss_batch)
        loss_mean = sum(loss_history)/(_idx_bt+1)
        # compute f1 score for the current batch
        f1_batch = binary_f1_score(train_batch_pred, _classes)
        f1_history.append(f1_batch)
        f1_mean = sum(f1_history)/(_idx_bt+1)
        # compute accuracy score for the current batch
        auc_batch = auc(train_batch_pred, _classes)
        auc_history.append(auc_batch)
        auc_mean = sum(auc_history)/(_idx_bt+1)
        pbar.set_postfix({'validation__loss': float(loss_mean),
                          'validation__f1_score': float(f1_mean),
                          'validation__auc': float(auc_mean)})
        pbar.update()

    print('### Validation report')
    print(f"### epoch {epoch}: loss {loss_mean} | f1_score {f1_mean} | auc_score {auc_mean} ###")
    return loss_mean, f1_mean, auc_mean



def train_one_epoch(model, optimizer: torch.optim.Adam, loss: torch.nn.modules.loss, 
                    training_loader, epoch:int):
    """
    """
    # this function trains just one epoch at time
    local_device = torch.device("cpu")
    loss_history     = []
    f1_history       = []
    auc_history = []
    # iterate over all the batches in the data loader
    pbar = tqdm(training_loader, position=0, leave=True)
    pbar.set_postfix({'training__loss': 0.0,
                      'training__f1_score': 0.0,
                      'training__auc': 0.0})
    for _idx_bt, _batch in enumerate(pbar):
    # for _batch in tqdm(training_loader):
        model.train()
        _real_batch, _position, _hash, _external_list, _function_edges, _classes = create_batch_data(one_batch=_batch)
        
        _real_batch = _real_batch.to(local_device)
        _position = torch.tensor(_position, dtype=torch.long).cpu()
        _classes = _classes.float().cpu()

        # forward pass
        train_batch_pred = model(local_method_batch=_real_batch, local_bt_positions=_position, 
                                 bt_external_names=_external_list, bt_all_method_edges=_function_edges, 
                                 local_device=local_device)
        # remove dimension 1 if needed 
        if len(train_batch_pred) > 1:
            train_batch_pred = train_batch_pred.squeeze()
        else:
            train_batch_pred = train_batch_pred[0]
        # print(train_batch_pred, "\n",_classes)
        # compute loss function for the current batch
        loss_batch = loss(train_batch_pred, _classes)
        loss_history.append(loss_batch)
        loss_mean = sum(loss_history)/(_idx_bt+1)
        # compute f1 score for the current batch
        f1_batch = binary_f1_score(train_batch_pred, _classes)
        f1_history.append(f1_batch)
        f1_mean = sum(f1_history)/(_idx_bt+1)
        # compute accuracy score for the current batch
        auc_batch = auc(train_batch_pred, _classes)
        auc_history.append(auc_batch)
        auc_mean = sum(auc_history)/(_idx_bt+1)
        pbar.set_postfix({'training__loss': float(loss_mean),
                          'training__f1_score': float(f1_mean),
                          'training__auc': float(auc_mean)})
        pbar.update()
        
        optimizer.zero_grad()
        # backward pass
        loss_batch.backward()
        # update the optimizer
        optimizer.step()

    print('### Training report')
    print(f"### epoch {epoch}: loss {loss_mean} | f1_score {f1_mean} | auc_score {auc_mean} ###")
    return loss_mean, f1_mean, auc_mean



def train_model():
    max_vocab_size = 10000
    vocab = Vocabulary(freq_file=external_fun_vocabulary, max_vocab_size=max_vocab_size)

    model = EmbedderGraphModel(external_vocab=vocab)

    # loss function
    binary_cross_entropy_loss = nn.BCELoss()
    
    lr = 1e-3
    wd = 1e-5
    # optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    optimizer = torch.optim.AdamW(model.parameters(), 
                                  lr=lr,
                                  weight_decay=wd)
    max_epochs = 20
    
    dataset_root_path = preprocess_path
    train_batch_size = 128
    valid_batch_size = 128


    train_dataset = PreprocessedDataset(root_dataset=dataset_root_path, flag='training')
    training_loader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True)

    valid_dataset = PreprocessedDataset(root_dataset=dataset_root_path, flag="valid")
    valid_loader = DataLoader(dataset=valid_dataset, batch_size=valid_batch_size)

    for epoch in range(max_epochs):
        # train for one epoch
        train_one_epoch(model,optimizer,binary_cross_entropy_loss,training_loader,epoch)   
        validate(model,binary_cross_entropy_loss,valid_loader,epoch) 



if __name__ == '__main__': 
    train_model()