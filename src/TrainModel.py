import torch
import sys
from Utils.vocabulary import Vocabulary
from Utils.config import external_fun_vocabulary,preprocess_path,max_epochs,max_vocab_size,lr,wd,train_batch_size,valid_batch_size,test_batch_size
from model.EmbedderGraphModel import EmbedderGraphModel
from Utils.preprocessedDataset import PreprocessedDataset,create_batch_data
from torch_geometric.loader import DataLoader
from torcheval.metrics.functional import binary_f1_score,auc
from torch import nn
from tqdm import tqdm
from sklearn import metrics
import numpy as np

import logging
logging.getLogger().setLevel(logging.ERROR)


def test(model,test_loader):
    local_device = torch.device("cpu")
    f1_history  = []
    auc_history = []

    for _idx_bt, _batch in enumerate(test_loader):
        model.eval()

        _real_batch, _position, _hash, _external_list, _function_edges, _classes = create_batch_data(one_batch=_batch)
        
        _real_batch = _real_batch.to(local_device)
        _position = torch.tensor(_position, dtype=torch.long).cpu()
        _classes = _classes.float().cpu()

        # forward pass
        test_batch_pred = model(local_method_batch=_real_batch, local_bt_positions=_position, 
                                 bt_external_names=_external_list, bt_all_method_edges=_function_edges, 
                                 local_device=local_device)


        # remove dimension 1 if needed 
        if len(test_batch_pred) > 1:
            test_batch_pred = test_batch_pred.squeeze()
        else:
            test_batch_pred = test_batch_pred[0]

        #trasform tensor in numpy array
        test_batch_pred = test_batch_pred.detach().cpu().numpy()
        _classes = _classes.detach().cpu().numpy()

        # compute f1 score for the current batch
        f1_batch = metrics.f1_score(_classes, np.round(test_batch_pred), average='macro')
        f1_history.append(float(f1_batch))
        f1_mean = sum(f1_history)/(_idx_bt+1)
        # compute accuracy score for the current batch
        fpr, tpr, thresholds = metrics.roc_curve(_classes,np.round(test_batch_pred))
        auc_batch = metrics.auc(fpr, tpr)
        auc_history.append(float(auc_batch))
        auc_mean = sum(auc_history)/(_idx_bt+1)


    print('### Test report')
    print(f"### f1_score {f1_mean} | auc_score {auc_mean} ###")



def validate(model, loss: torch.nn.modules.loss, valid_loader, epoch:int):
    """
    """
    local_device = torch.device("cpu")
    loss_history     = []
    f1_history       = []
    auc_history = []

    # iterate over all the batches in the data loader
    pbar = tqdm(valid_loader, position=0, leave=True)
    pbar.set_postfix({'validation__loss': 0.0,
                      'validation__f1_score': 0.0,
                      'validation__auc': 0.0})
    
    for _idx_bt, _batch in enumerate(pbar):
    # for _batch in tqdm(training_loader):
        model.eval()
        _real_batch, _position, _hash, _external_list, _function_edges, _classes = create_batch_data(one_batch=_batch)
        
        _real_batch = _real_batch.to(local_device)
        _position = torch.tensor(_position, dtype=torch.long).cpu()
        _classes = _classes.float().cpu()

        # forward pass
        train_batch_pred = model(local_method_batch=_real_batch, local_bt_positions=_position, 
                                 bt_external_names=_external_list, bt_all_method_edges=_function_edges, 
                                 local_device=local_device)
        # remove dimension 1 if needed 
        if len(train_batch_pred) > 1:
            train_batch_pred = train_batch_pred.squeeze()
        else:
            train_batch_pred = train_batch_pred[0]

        # compute loss function for the current batch
        loss_batch = loss(train_batch_pred, _classes)
        loss_history.append(loss_batch)
        loss_mean = sum(loss_history)/(_idx_bt+1)
        # compute f1 score for the current batch
        # f1_batch = binary_f1_score(train_batch_pred, _classes)
        train_batch_pred = train_batch_pred.detach().cpu().numpy()
        _classes = _classes.detach().cpu().numpy()
        f1_batch = metrics.f1_score(_classes, np.round(train_batch_pred), average='macro')
        f1_history.append(float(f1_batch))
        f1_mean = sum(f1_history)/(_idx_bt+1)
        # compute accuracy score for the current batch
        # auc_batch = auc(train_batch_pred, _classes)
        # auc_history.append(auc_batch)
        # auc_mean = sum(auc_history)/(_idx_bt+1)


        fpr, tpr, thresholds = metrics.roc_curve(_classes,np.round(train_batch_pred))
        auc_batch = metrics.auc(fpr, tpr)
        auc_history.append(float(auc_batch))
        auc_mean = sum(auc_history)/(_idx_bt+1)
        pbar.set_postfix({'validation__loss': float(loss_mean),
                          'validation__f1_score': float(f1_mean),
                          'validation__auc': float(auc_mean)})
        pbar.update()

    print('### Validation report')
    print(f"### epoch {epoch}: loss {loss_mean} | f1_score {f1_mean} | auc_score {auc_mean} ###")
    return loss_mean, f1_mean, auc_mean



def train_one_epoch(model, optimizer: torch.optim.Adam, loss: torch.nn.modules.loss, 
                    training_loader, epoch:int):
    """
    """
    # this function trains just one epoch at time
    local_device = torch.device("cpu")
    loss_history     = []
    f1_history       = []
    auc_history = []
    # iterate over all the batches in the data loader
    pbar = tqdm(training_loader, position=0, leave=True)
    pbar.set_postfix({'training__loss': 0.0,
                      'training__f1_score': 0.0,
                      'training__auc': 0.0})
    for _idx_bt, _batch in enumerate(pbar):
    # for _batch in tqdm(training_loader):
        model.train()
        _real_batch, _position, _hash, _external_list, _function_edges, _classes = create_batch_data(one_batch=_batch)
        
        _real_batch = _real_batch.to(local_device)
        _position = torch.tensor(_position, dtype=torch.long).cpu()
        _classes = _classes.float().cpu()

        # forward pass
        train_batch_pred = model(local_method_batch=_real_batch, local_bt_positions=_position, 
                                 bt_external_names=_external_list, bt_all_method_edges=_function_edges, 
                                 local_device=local_device)
        # remove dimension 1 if needed 
        if len(train_batch_pred) > 1:
            train_batch_pred = train_batch_pred.squeeze()
        else:
            train_batch_pred = train_batch_pred[0]
        # print(train_batch_pred, "\n",_classes)
        # compute loss function for the current batch
        loss_batch = loss(train_batch_pred, _classes)
        loss_history.append(loss_batch)
        loss_mean = sum(loss_history)/(_idx_bt+1)
        # compute f1 score for the current batch
        # f1_batch = binary_f1_score(torch.round(train_batch_pred), _classes)
        train_batch_pred = train_batch_pred.detach().cpu().numpy()
        _classes = _classes.detach().cpu().numpy()
        f1_batch = metrics.f1_score(_classes, np.round(train_batch_pred), average='macro')
        f1_history.append(float(f1_batch))
        f1_mean = sum(f1_history)/(_idx_bt+1)
        # compute accuracy score for the current batch
        # auc_batch = auc(train_batch_pred, _classes)
        # auc_history.append(float(auc_batch))
        # auc_mean = sum(auc_history)/(_idx_bt+1)

        
        fpr, tpr, thresholds = metrics.roc_curve(_classes,np.round(train_batch_pred))
        auc_batch = metrics.auc(fpr, tpr)
        auc_history.append(float(auc_batch))
        auc_mean = sum(auc_history)/(_idx_bt+1)
        pbar.set_postfix({'training__loss': float(loss_mean),
                          'training__f1_score': float(f1_mean),
                          'training__auc': float(auc_mean)})
        pbar.update()
        
        optimizer.zero_grad()
        # backward pass
        loss_batch.backward()
        # update the optimizer
        optimizer.step()

    print('### Training report')
    print(f"### epoch {epoch}: loss {loss_mean} | f1_score {f1_mean} | auc_score {auc_mean} ###")
    return loss_mean, f1_mean, auc_mean



def train_model(model,max_epochs):
    # loss function
    binary_cross_entropy_loss = nn.BCELoss()

    #optimezer
    # lr = 1e-3
    # wd = 1e-5
    # optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    optimizer = torch.optim.AdamW(model.parameters(), 
                                  lr=lr,
                                  weight_decay=wd)
   
    
    train_dataset = PreprocessedDataset(root_dataset=preprocess_path, flag='training')
    training_loader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True)

    valid_dataset = PreprocessedDataset(root_dataset=preprocess_path, flag="valid")
    valid_loader = DataLoader(dataset=valid_dataset, batch_size=valid_batch_size)

    for epoch in range(max_epochs):
        # train for one epoch
        train_one_epoch(model,optimizer,binary_cross_entropy_loss,training_loader,epoch)   
        validate(model,binary_cross_entropy_loss,valid_loader,epoch) 


    torch.save(model.state_dict(), "./model.pt")


if __name__ == '__main__': 
    local_device = torch.device("cpu")
    # max_epochs = 5
    # max_vocab_size = 10000
    # dataset_root_path = preprocess_path
    # train_batch_size = 128
    # valid_batch_size = 128
    # test_batch_size = 128

    vocab = Vocabulary(freq_file=external_fun_vocabulary, max_vocab_size=max_vocab_size)
    model = EmbedderGraphModel(external_vocab=vocab)

    if str(sys.argv[1]) == "--train":
        train_model(model,max_epochs)
    else:
        model.load_state_dict(torch.load("./model.pt",map_location=local_device))

        test_dataset = PreprocessedDataset(root_dataset=preprocess_path, flag="test")
        test_loader = DataLoader(dataset=test_dataset, batch_size=test_batch_size)
        test(model,test_loader)