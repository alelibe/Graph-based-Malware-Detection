import os
import torch
import pandas as pd
from datetime import datetime
from Utils.config import test_csv
from torch_geometric.data import Batch
from torch_geometric.loader import DataLoader
from torch_geometric.data import Dataset

def create_batch_data(one_batch: Batch):
    acfg_list = []
    acfg_position = [0]
    acfg_count = 0
    
    assert len(one_batch.external_list) == len(one_batch.function_edges) == len(one_batch.local_acfgs) == len(one_batch.hash), "size of each component must be equal to each other"
    
    for item in one_batch.local_acfgs:
        for acfg in item:
            # print("acfg: ", acfg)
            acfg_list.append(acfg)
        acfg_count += len(item)
        # print("acfg_count ", acfg_count)
        acfg_position.append(acfg_count)
    
    if len(one_batch.local_acfgs) == 1 and len(one_batch.local_acfgs[0]) == 0:
        return (None for _ in range(6))
    else:
        acfg_batch = Batch.from_data_list(acfg_list)
        return acfg_batch, acfg_position, one_batch.hash, one_batch.external_list, one_batch.function_edges, one_batch.targets
    


class PreprocessedDataset(Dataset):
    def __init__(self, root_dataset, flag, transform=None, pre_transform=None, date='all'):
        super(PreprocessedDataset, self).__init__(None, transform, pre_transform)
        # build the name of the correct folder: trainingSet, testSet or validSet
        self.flag = flag.lower()
        self.date=date
        self.rootDS = os.path.join(root_dataset,flag+"Set").replace("\\","/")
        if date=='all':
            self.rootDS_files = os.listdir(self.rootDS)
        else:
            self.rootDS_files = os.listdir(self.rootDS)
            df_test = pd.read_csv(test_csv)
            df_test_date = df_test[df_test['timestamp']==date]
            for f in self.rootDS_files:
                index_pt = f.split(".")[0]
                if not int(index_pt) in df_test_date["index_file"].values:
                    self.rootDS_files.remove(f)

            # df_test = pd.read_csv(test_csv)
            # test_files = os.listdir(self.rootDS)
            # print(test_files[:10])
            # df_test_date = df_test[df_test['timestamp']==date]
            # # test_files_path = [os.path.join(self.rootDS, str(f)+'.pt').replace("\\","/") 
            # #                      for f in df_test_date['index_file'].to_list()]
            # self.rootDS_files = [str(f)+'.pt' for f in df_test_date['index_file'].to_list()]
            # for f in self.rootDS_files:
            #     # print(f.split("/")[-1])
            #     if not f in test_files:
            #         print("remove", f)
            #         self.rootDS_files.remove(f)
            # # print(self.rootDS_files)
    
    @staticmethod
    def _list_files_for_pt(the_path):
        files = []
        for name in os.listdir(the_path):
            if os.path.splitext(name)[-1] == '.pt':
                files.append(name)
        return files
    
    def len(self):
        return len(self.rootDS_files)
    
    def get(self, idx):
        idx_infolder = self.rootDS_files[idx]
        # idx_data = torch.load(os.path.join(self.rootDS, "{}.pt".format(idx+1)).replace("\\","/"))
        idx_data = torch.load(os.path.join(self.rootDS, idx_infolder).replace("\\","/"))
        return idx_data
        # print(idx_infolder)
        # return idx_infolder


# def _simulating(_dataset, _batch_size: int):
#     print("\nBatch size = {}".format(_batch_size))
#     time_start = datetime.now()
#     print("start time: " + time_start.strftime("%Y-%m-%d@%H:%M:%S"))
    
#     # https://github.com/pytorch/fairseq/issues/1560
#     # https://github.com/pytorch/pytorch/issues/973#issuecomment-459398189
#     # loaders_1 = DataLoader(dataset=benign_exe_dataset, batch_size=10, shuffle=True, num_workers=0)
#     # increasing the shared memory: ulimit -SHn 51200
#     loader = DataLoader(dataset=_dataset, batch_size=_batch_size, shuffle=True)  # default of prefetch_factor = 2 # num_workers=4
    
#     for index, data in enumerate(loader):
#         if index >= 3:
#             break
#         _real_batch, _position, _hash, _external_list, _function_edges, _true_classes = create_batch_data(one_batch=data)
#         print(data)
#         print("Real Batch: ", _real_batch)
#         print("Hash: ", _hash)
#         print("Position: ", _position)
#         print("external_list: ", _external_list)
#         print("function_edge: ", _function_edges)
#         print("true_classes: ", _true_classes)
#         print("\n")
    
#     time_end = datetime.now()
#     print("end time: " + time_end.strftime("%Y-%m-%d@%H:%M:%S"))
#     print("All time = {}\n\n".format(time_end - time_start))


# if __name__ == '__main__':
#     root_path: str = 'C:/Users/Alessia/Desktop/Unimi/Tesi/Preprocess/pyG_prova/'
#     i_batch_size = 3
    
#     train_dataset = PreprocessedDataset(root_dataset=root_path, train_or_test='train')
#     print(train_dataset.rootDS_files)
#     print(len(train_dataset.rootDS_files), len(train_dataset))
#     _simulating(_dataset=train_dataset, _batch_size=i_batch_size)
    
    # valid_dataset = MalwareDetectionDataset(root=root_path, train_or_test='valid')
    # print(valid_dataset.malware_root, valid_dataset.benign_root)
    # print(len(valid_dataset.malware_files), len(valid_dataset.benign_files), len(valid_dataset))
    # _simulating(_dataset=valid_dataset, _batch_size=i_batch_size)
    
    # test_dataset = MalwareDetectionDataset(root=root_path, train_or_test='test')
    # print(test_dataset.malware_root, test_dataset.benign_root)
    # print(len(test_dataset.malware_files), len(test_dataset.benign_files), len(test_dataset))
    # _simulating(_dataset=test_dataset, _batch_size=i_batch_size)