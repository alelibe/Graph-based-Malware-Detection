# This code trasform each graph (stored in a json file) in a pyg object
from Utils.config import preprocessedGraphs,external_fun_vocabulary,pyg_objects,preprocess_path,tesseract
from Utils.vocabulary import Vocabulary
from Utils.trasformGraphs import graph_list_2_pyg_object
import pandas as pd
import shutil
import os
import math
import sys
from sklearn.model_selection import train_test_split


def split_dataset():
    csv_path = preprocess_path +"dataset.csv"
    df = pd.read_csv(csv_path)
    # df.drop(columns=df.columns[0], axis=1, inplace=True)
    df_train, df_test = train_test_split(df, test_size=0.33, random_state=42)
    df_train, df_valid = train_test_split(df_train, test_size=0.20, random_state=42)
    df_train.to_csv(preprocess_path+"train_set.csv",index=False)
    df_valid.to_csv(preprocess_path+"valid_set.csv",index=False)
    df_test.to_csv(preprocess_path+"test_set.csv",index=False)


    if not os.path.exists(preprocess_path+"TrainingSet"):
        os.mkdir(preprocess_path+"TrainingSet")
    if not os.path.exists(preprocess_path+"ValidSet"):
        os.mkdir(preprocess_path+"ValidSet")
    if not os.path.exists(preprocess_path+"TestSet"):
        os.mkdir(preprocess_path+"TestSet")

    # moveTrainFile = lambda x: shutil.move(pyg_objects+str(x)+".pt",preprocess_path+"TrainingSet/"+str(x)+".pt")
    copyTrainFile = lambda x: shutil.copy(pyg_objects+str(x)+".pt",preprocess_path+"TrainingSet/"+str(x)+".pt")
    df_train["index_file"].apply(copyTrainFile)

    # moveValidFile = lambda x: shutil.move(pyg_objects+str(x)+".pt",preprocess_path+"ValidSet/"+str(x)+".pt")
    copyValidFile = lambda x: shutil.copy(pyg_objects+str(x)+".pt",preprocess_path+"ValidSet/"+str(x)+".pt")
    df_valid["index_file"].apply(copyValidFile)
    
    # moveTestFile = lambda x: shutil.move(pyg_objects+str(x)+".pt",preprocess_path+"TestSet/"+str(x)+".pt")
    copyTestFile = lambda x: shutil.copy(pyg_objects+str(x)+".pt",preprocess_path+"TestSet/"+str(x)+".pt")
    df_test["index_file"].apply(copyTestFile)


def constraint1(df):
    df_train = df[df["timestamp"] <= "2015"]
    df_train.to_csv(tesseract+"train_set.csv",index=False)
    df_test = df[df["timestamp"] > "2015"] 
    df_test.to_csv(tesseract+"test_set.csv",index=False)

    return df_train,df_test


def constrain23(df_train,df_test,delta = 10):
    # CONTRAINT 2
    # count of goodware and malware for each <'timestamp','label'> in trainSet
    df_count_train = df_train.groupby(['timestamp','label']).count()
    df_count_train = df_count_train.reset_index()
    # for each unique timestamp in the resulting count
    for ts in df_count_train['timestamp'].unique():
        # if the len of the dataframe (i.e. the number of class) is 1 it means that
        # it has only goodware or only malware, violating cons 2 
        if len(df_count_train[df_count_train['timestamp']==ts]) == 1:
            # drop the timestamp rows with only goodware or only malware
            df_train = df_train[df_train['timestamp'] != ts]

    # count of goodware and malware for each <'timestamp','label'> in testSet
    df_count_test = df_test.groupby(['timestamp','label']).count() 
    df_count_test = df_count_test.reset_index()
    # for each unique timestamp in the resulting count
    for ts in df_count_test['timestamp'].unique():
        # if the len of the dataframe (i.e. the number of class) is 1 it means that
        # it has only goodware or only malware violating cons 2 
        df_timestamp = df_count_test[df_count_test['timestamp']==ts]
        if len(df_timestamp) == 1:
            # drop the timestamp rows with only goodware or only malware
            df_test = df_test[df_test['timestamp'] != ts]
        # CONTRAINT 3
        sum_ts = (df_timestamp["index_file"]).sum()
        num_mw_ideal = int(math.floor((sum_ts*delta)/100))
        num_mw_actual = df_timestamp[df_timestamp["label"]==1]["index_file"].iloc[0]
        # print(num_mw_ideal, num_mw_actual)
        if num_mw_ideal > num_mw_actual:
            # it means that there are too many goodware respect to malware, so we have to delete some of them
            num_gw_ideal  = int(math.floor((num_mw_actual*(100-delta))/delta))
            num_gw_actual = df_timestamp[df_timestamp["label"]==0]["index_file"].iloc[0]
            gw_to_delete = num_gw_actual - num_gw_ideal
            df_test = delete_gw_or_mw(df_test, gw_to_delete, ts, True)
        elif num_mw_ideal < num_mw_actual:
            # it means that there are too many malware respect to goodware, so we have to delete some of them
            mw_to_delete = num_mw_actual - num_mw_ideal
            df_test = delete_gw_or_mw(df_test,mw_to_delete,ts,False)

    return df_train,df_test


def split_with_tess():
    csv_path = preprocess_path +"dataset.csv"
    df = pd.read_csv(csv_path)

    if not os.path.exists(tesseract+"trainingSet"):
        os.mkdir(tesseract+"trainingSet_tess")
    if not os.path.exists(tesseract+"testSet"):
        os.mkdir(tesseract+"testSet_tess")

    # TESSERACT FRAMEWORK: constraint 1
    df_train, df_test = constraint1(df)
    print("---CONTRAINT 1 VERIFIED---")

    # TESSERACT FRAMEWORK: constraint 2 and 3
    df_train, df_test = constrain23(df_train,df_test)
    print("---CONTRAINT 2 AND 3 VERIFIED---")

    copyTrainFile = lambda x: shutil.copy(pyg_objects+str(x)+".pt",tesseract+"trainingSet/"+str(x)+".pt")
    df_train["index_file"].apply(copyTrainFile)

    copyTestFile = lambda x: shutil.copy(pyg_objects+str(x)+".pt",tesseract+"testSet/"+str(x)+".pt")
    df_test["index_file"].apply(copyTestFile)



def delete_gw_or_mw(df: pd.DataFrame, to_delete: int, timestamp: str, F_DELETE_GW: bool = True):
    if F_DELETE_GW:
        idx_todelete = df[(df['timestamp']==timestamp) & (df['label']==0)].sample(to_delete)
    else:
        idx_todelete = df[(df['timestamp']==timestamp) & (df['label']==1)].sample(to_delete)
    df = df.drop(idx_todelete.index)
    return df
        
        

if __name__ == '__main__':
    # transfrom graphs of each apk to one graph
    # graphAggregator()

    max_vocab_size = 10000
    vocabulary = Vocabulary(freq_file=external_fun_vocabulary, max_vocab_size=max_vocab_size)

    #convert json graph in pyG object
    # graph_list_2_pyg_object(dir_graphs=preprocessedGraphs,label=1,vocab=vocabulary)

    if str(sys.argv[1]) == "--tess":
        split_with_tess()
    else:
        split_dataset()